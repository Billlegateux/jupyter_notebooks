{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources:**<br>\n",
    "- [10-minute guide to data modeling](https://www.credera.com/blog/technology-solutions/data-modeling-explained-in-10-minutes-or-less/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization versus De-Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalization: breaking a table down into more tables to reduce redundancy and maintain data integrity\n",
    "- De-Normalization: not breaking a table into smaller tables, consolidating to fewer tables, risk for data redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 3 Normal Forms or 3NF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1st Normal Form: Remove duplicate data and break data down to granular or more atomic level\n",
    "- 2nd Normal Form: All columns in a table must depend on the primary key column\n",
    "- 3rd Normal Form: A column should not be duplicated or exists in multiple tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are higher normal forms, but achieving 3NF is \"good enough\" for most real world cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not in 3NF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Poor_Design.png \"Bad Design\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In 3NF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Better_Design.png \"Bad Design\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star vs Snowflake Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- [Star vs Snowflake Schema](https://www.vertabelo.com/blog/technical-articles/data-warehouse-modeling-star-schema-vs-snowflake-schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Star Schema Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Star_Schema_Summary.png \"Star Schema Summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Star Schema Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Star_Schema.png \"Star Schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowflake Schema Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Snow_Flake_Summary.png \"Snow Flake Summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowflake Schema Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Snow_Flake_Schema.png \"Snow Flake Schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Designing a traditional relational database](https://www.youtube.com/watch?v=I_rxqSJAj6U)\n",
    "- [Data Warehouse Design](https://www.youtube.com/watch?v=--OJpdPeH80)\n",
    "- [ETL Design](https://youtu.be/sLhInuwdwcc)\n",
    "- [Star Schema vs Snowflake](https://youtu.be/Qq4yhhAk9fc)\n",
    "- [OLAP vs OLTP](https://youtu.be/AiZWeSUjylU)\n",
    "- [Slowly Changing Dimensions](https://youtu.be/1FZ7et0pN4c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectural Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Architectural Considerations](https://medium.com/ssense-tech/principled-data-engineering-part-i-architectural-overview-6d4bdf89b657)\n",
    "- Data Mart\n",
    "- Database\n",
    "- Data Warehouse\n",
    "- Data Lake\n",
    "    - **Raw Data:** A “raw data” bucket which contains the raw,dumps untransformed, and lossless incoming data from all our sources such as event streams from microservices, transactional database snapshots, log dumps, and data from third party sources via FTP or API calls. Data here can be in various formats such as CSV, JSON, flat text files etc., and may not have defined schemas. This bucket plays the key role of guaranteeing no data loss and replayability of our pipelines. In other words, if our pipelines downstream change or fail, our raw data store guarantees that all the original source data remains intact and available for re-processing.\n",
    "    - **Interim Data:** An “interim data” bucket which imports the aforementioned raw data and performs the most minimal transformations required to homogenize its structure, impose schemas, and allow cataloging. Recommend using a format that enforces a schema like parquet. This eliminates a lot of dangerous ambiguities of schema-less data, which can lead to data loss and poor governance. The minimal transformations performed at this step also allow for some critical type management such as homogenizing date formats and number types (decimals, floats, doubles, etc.), and handling null values.\n",
    "    - **Business Data:** A “business data” bucket which presents transformed datasets to end-users. Data here conforms to semantically meaningful naming conventions and each dataset corresponds to a specific business need. Furthermore, the datasets here have more refined schemas that make sense to our end-users — the consumers of this data.\n",
    "- Pipeline\n",
    "    - Desired characteristics:\n",
    "        - Replayable or reproducible\n",
    "        - Has idempotency: applied several times without changing the result beyond the initial application to prevent duplicate or corrupt data\n",
    "    - Types of pipelines:\n",
    "        - ETL: In an ETL pipeline, data is extracted from a source, transformed to the required shape, and inserted into the target. The advantage of ETL is that data enters your system in the shape you want it to be in, and can easily be modeled for analytics. This works exceedingly well when the data is coming from consistent and trusted sources, but can quickly become too brittle in cases where there is a possibility of a schema change, or the data cannot be re-extracted. Imagine a scenario wherein data is extracted from a third-party API, transformed and then loaded into a data warehouse. Some time later the process needs to be rerun, but the source data is no longer available due to the third-party aggregating its data after a certain amount of time (to reduce storage fees). In other words, your system does not offer immutable data.\n",
    "        - ELT: ELT addresses this by extracting and loading the raw data immediately into storage. From there, you can rerun the transformation portion of the pipelines to your heart’s content. The drawback here is that the raw data can potentially be schemaless and unstructured. Managing this raw data becomes the main difficulty in this configuration, and in an era of ever increasing compliance, proper cataloging is critical.\n",
    "        - Pipeline \"as code\" (use of programming languages)\n",
    "        - GUI pipeline tools (Informatica, Talend, Pentaho, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Governance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Data Governance](https://medium.com/ssense-tech/principled-data-engineering-part-ii-data-governance-30297abb2446)\n",
    "- Metadata Management\n",
    "    - Data Cataloging: the \"what\" of your data: table names, column names, data type, etc\n",
    "    - Data Lineage: the \"where\" of your data: what are the sources of your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLAP - Online Analytical Processing System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Typical of STAR Schema\n",
    "- De-normalized (dimension and fact design)\n",
    "- Faster analysis and search by combining tables\n",
    "- Requires more data storage due to redundancy\n",
    "- Less complex join complexity compared to OLTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLTP - Online Transaction Processing System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Typical of Snow Flake Schema\n",
    "- Normalized (1st to 3rd normal form)\n",
    "- Faster inserts, updates, deletes, and improved data quality due to reduced redundancy\n",
    "- Requires less space due to reduction of redundancy\n",
    "- Query performance slower compared to OLAP or STAR Schema due to more tables to scan\n",
    "- Join complexity increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Ecosystem - \"Old\" Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The 3 Main Layers**\n",
    "- **HDFS:** Hadoop Distributed File System (storage layer) \n",
    "- **Yarn:** Yet Another Resource Negotiator: How to get all these computers to work together in an \n",
    "  orderly fashion (orchestration layer)\n",
    "- **MapReduce:** How to do stuff with the distributed data in a cluster (execution layer / Java code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Storage layer:** RDD, Apache Arrow\n",
    "- **Query / ETL layer:** Spark SQL, Spark Dataframe, Apache Beam, Apache Pig, Apache Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Web Services (AWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Storage layer:** S3 buckets\n",
    "- **Query / ETL layer:** Athena (SQL syntax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ETL tools\n",
    "    - AWS Glue\n",
    "    - Apache Beam\n",
    "    - Apache Spark\n",
    "    - Dask\n",
    "    - Apache Airflow (Python)\n",
    "    - Prefect (Python)\n",
    "    - Papermill (Python)\n",
    "    - SAS\n",
    "    - IBM Informatica\n",
    "    - Talend\n",
    "    - Pentaho\n",
    "- SQL-like tools for Big Data\n",
    "    - AWS Athena / PyAthena\n",
    "    - Apache Spark SQL\n",
    "    - Apache Hive\n",
    "- Dataframe-like tools for Big Data\n",
    "    - Spark dataframe\n",
    "    - [Koalas](https://github.com/databricks/koalas)\n",
    "    - Dask dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data or Storage Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema-less formats\n",
    "    - HDFS\n",
    "    - S3\n",
    "    - RDD\n",
    "Schema formats\n",
    "    - parquet (column oriented, on-disc)\n",
    "    - avro (row oriented, on-disc)\n",
    "    - orc (column oriented, on-disc)\n",
    "    - arrow (column oriented, main memory store)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization versus De-Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalization: breaking a table down into more tables to reduce redundancy and maintain data integrity\n",
    "- De-Normalization: not breaking a table into smaller tables, risk for data redundancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 3 Normal Forms or 3NF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1st Normal Form: Remove duplicate data and break data down to granular or more atomic level\n",
    "- 2nd Normal Form: All columns in a table must depend on the primary key column\n",
    "- 3rd Normal Form: A column should not be duplicated or exists in multiple tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not in 3NF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Poor_Design.png \"Bad Design\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In 3NF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/Better_Design.png \"Bad Design\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Designing a traditional relational database](https://www.youtube.com/watch?v=I_rxqSJAj6U)\n",
    "- [Data Warehouse Design](https://www.youtube.com/watch?v=--OJpdPeH80)\n",
    "- [ETL Design](https://youtu.be/sLhInuwdwcc)\n",
    "- [Star Schema vs Snowflake](https://youtu.be/Qq4yhhAk9fc)\n",
    "- [OLAP vs OLTP](https://youtu.be/AiZWeSUjylU)\n",
    "- [Slowly Changing Dimensions](https://youtu.be/1FZ7et0pN4c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectural Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Architectural Considerations](https://medium.com/ssense-tech/principled-data-engineering-part-i-architectural-overview-6d4bdf89b657)\n",
    "- Database\n",
    "- Data Warehouse\n",
    "- Data Lake\n",
    "    - **Raw Data:** A “raw data” bucket which contains the raw, untransformed, and lossless incoming data from all our sources such as event streams from microservices, transactional database snapshots, log dumps, and data from third party sources via FTP or API calls. Data here can be in various formats such as CSV, JSON, flat text files etc., and may not have defined schemas. This bucket plays the key role of guaranteeing no data loss and replayability of our pipelines. In other words, if our pipelines downstream change or fail, our raw data store guarantees that all the original source data remains intact and available for re-processing.\n",
    "    - **Interim Data:** An “interim data” bucket which imports the aforementioned raw data and performs the most minimal transformations required to homogenize its structure, impose schemas, and allow cataloging. Recommend using a format that enforces a schema like parquet. This eliminates a lot of dangerous ambiguities of schema-less data, which can lead to data loss and poor governance. The minimal transformations performed at this step also allow for some critical type management such as homogenizing date formats and number types (decimals, floats, doubles, etc.), and handling null values.\n",
    "    - **Business Data:** A “business data” bucket which presents transformed datasets to end-users. Data here conforms to semantically meaningful naming conventions and each dataset corresponds to a specific business need. Furthermore, the datasets here have more refined schemas that make sense to our end-users — the consumers of this data.\n",
    "- Pipeline\n",
    "    - Desired characteristics:\n",
    "        - Replayable or reproducible\n",
    "        - Has idempotency: applied several times without changing the result beyond the initial application to prevent duplicate or corrupt data\n",
    "    - Types of pipelines:\n",
    "        - ETL: In an ETL pipeline, data is extracted from a source, transformed to the required shape, and inserted into the target. The advantage of ETL is that data enters your system in the shape you want it to be in, and can easily be modeled for analytics. This works exceedingly well when the data is coming from consistent and trusted sources, but can quickly become too brittle in cases where there is a possibility of a schema change, or the data cannot be re-extracted. Imagine a scenario wherein data is extracted from a third-party API, transformed and then loaded into a data warehouse. Some time later the process needs to be rerun, but the source data is no longer available due to the third-party aggregating its data after a certain amount of time (to reduce storage fees). In other words, your system does not offer immutable data.\n",
    "        - ELT: ELT addresses this by extracting and loading the raw data immediately into storage. From there, you can rerun the transformation portion of the pipelines to your heart’s content. The drawback here is that the raw data can potentially be schemaless and unstructured. Managing this raw data becomes the main difficulty in this configuration, and in an era of ever increasing compliance, proper cataloging is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Governance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Data Governance](https://medium.com/ssense-tech/principled-data-engineering-part-ii-data-governance-30297abb2446)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Typical of STAR Schema\n",
    "- De-normalized (dimension and fact design)\n",
    "- Faster analysis and search by combining tables\n",
    "- Requires more data storage due to redundancy\n",
    "- Simpler joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Typical of Snowflake Schema\n",
    "- Normalized (1st to 3rd normal form)\n",
    "- Faster inserts, updates, deletes, and improved data quality by reducing redundancy\n",
    "- Requires less space due to reduction of redundancy\n",
    "- Performance not as great as OLAP or STAR Schema due to more tables\n",
    "- More complex joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ETL tools\n",
    "    - AWS Glue\n",
    "    - Apache Beam\n",
    "    - Apache Airflow (Python)\n",
    "    - Prefect (Python)\n",
    "    - Papermill (Python)\n",
    "- SQL-like tools\n",
    "    - AWS Athena / PyAthena\n",
    "    - Apache Spark SQL\n",
    "    - Apache Hive\n",
    "- Dataframe-like\n",
    "    - PySpark dataframe\n",
    "    - [Koalas](https://github.com/databricks/koalas)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

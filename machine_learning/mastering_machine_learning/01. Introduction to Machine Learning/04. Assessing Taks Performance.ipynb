{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing task performance\n",
    "\n",
    "In order for the machine to learn, there will be some objectively quantifiable measure of performance. The machine must know what the word \"improve\" means. For instance, most college professors assess the performance of their students by giving them exams. These exams are usually given a grade out of 100 points, where a higher grade translates to better exam (task) performance.\n",
    "\n",
    "## Assessing regression task performance\n",
    "\n",
    "There are numerous performance metrics that have been devised for both regression and classification tasks. Typically, with regression tasks, we are interested in getting as close to the target variable as possible. The difference between the prediction made by the machine learning model and the ground truth is called the **error** or **residual**.  For this rest of this chapter, we will focus on assessing task performance. Let's begin by reading in our housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "housing = pd.read_csv('../data/housing_sample.csv')\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous models\n",
    "\n",
    "Let's recreate our simple models that we built previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_price = housing['SalePrice'].mean()\n",
    "price_per_sq_foot = (housing['SalePrice'] / housing['GrLivArea']).mean()\n",
    "group_price = [housing.query('GrLivArea < 1500')['SalePrice'].mean(),\n",
    "               housing.query('1500 <= GrLivArea <= 2000')['SalePrice'].mean(),\n",
    "               housing.query('GrLivArea > 2000')['SalePrice'].mean()]\n",
    "\n",
    "def model_1_simple_avg(sq_foot):\n",
    "    return round(mean_price, -3)\n",
    "\n",
    "def model_2_avg_ppsf(sq_foot):\n",
    "    return round(price_per_sq_foot * sq_foot, -3)\n",
    "\n",
    "def model_3_bin_avg(sq_foot):\n",
    "    cat = 0 if sq_foot < 1500 else 1 if sq_foot <= 2000 else 2\n",
    "    return group_price[cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to make prediction\n",
    "\n",
    "To automate the process of predicting, let's create a function that accepts the model and data as inputs and outputs the predicted values. We return the values as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def make_prediction(model, X):\n",
    "    return np.array([model(x) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a list of house square footages we would like to use to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1200, 1800, 2750]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass one of our models along with this data to the `make_prediction` function to return the predicted sale prices. Here, we use the second model that uses average price per square foot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_prediction(model_2_avg_ppsf, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on labeled data\n",
    "\n",
    "If we want to test how well our model is performing, we need to make predictions on data where the sale price is known. Let's use our existing housing data and make a prediction for each home. The first five predicted values are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing['GrLivArea']\n",
    "y_pred = make_prediction(model_2_avg_ppsf, X)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this data is labeled, we can output the ground truth to see how close our predictions were to the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = housing['SalePrice']\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the predictions and actual values\n",
    "\n",
    "It can be instructive to plot the actual outcome and the value predicted by the model on the same plot with a line connecting the two points denoting the error. The following function plots 20 random points (by default) along with their predicted value and error given one of the above models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_pred_vs_actual(model, n=20):\n",
    "    housing_sample = housing.sample(n, random_state=123)\n",
    "    X = housing_sample['GrLivArea']\n",
    "    y = housing_sample['SalePrice']\n",
    "    y_pred = make_prediction(model, X)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    ax.scatter(X, y, marker='o', label='Ground Truth')\n",
    "    ax.scatter(X, y_pred, label='Prediction')\n",
    "    ax.vlines(X, y, y_pred, linestyle='--', label='error')\n",
    "    \n",
    "    ax.set_title(f\"Random {n} points from {model.__name__}\")\n",
    "    ax.set_xlabel('Square Feet')\n",
    "    ax.set_ylabel('Sale Price')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_vs_actual(model_1_simple_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_vs_actual(model_2_avg_ppsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_vs_actual(model_3_bin_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing model performance\n",
    "\n",
    "Once we have our models, we can determine which one has the best performance. There are several metrics we can use to compare the models against one another. Here are some example metrics that we could report for each model:\n",
    "\n",
    "* Calculate the absolute value of all the errors and sum them\n",
    "* Calculate the single largest absolute error\n",
    "* Square all the errors and then sum these squared values\n",
    "* Cube all the errors, and then sum the absolute value of all these cubed values\n",
    "* Take the log of all the absolute value of errors and sum all these logged values\n",
    "\n",
    "For all of these metrics, the model with the lowest value would be classified as the best. There is no limit to the number of error metrics that can be devised. As long as they produce a single statistic that we can use to rank the performance of each model, then it is a valid metric.\n",
    "\n",
    "### Sum of squared error\n",
    "\n",
    "The most common approach is to use the squared error. Adding all of the squared errors together yields what is known as the **sum of squared error** and is often abbreviated as SSE. Let's calculate the SSE of the second model on the first five values. We first calculate the actual (raw) error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing['GrLivArea']\n",
    "y = housing['SalePrice']\n",
    "y_pred = make_prediction(model_2_avg_ppsf, X)\n",
    "error = y[:5] - y_pred[:5]\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we find the raw error, we square each term, making each value positive (and very large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_error = error ** 2\n",
    "squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing these squared errors gives us our error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_squared_error = squared_error.sum()\n",
    "sum_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models\n",
    "\n",
    "Having this one metric to objectively quantify our models makes for easy comparison. Let's write a function that computes the SSE on all the data for any model. It accepts the input and output data along with the model. In the function, the predicted values are calculated. From here, the raw error is found, then squared, summed and returned as our metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse(X, y, model):\n",
    "    y_pred = make_prediction(model, X)\n",
    "    error = y - y_pred\n",
    "    squared_error = error ** 2\n",
    "    return squared_error.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this function to calculate the SSE for all three of the models. We return a pandas Series mapping the name of the function (using the special attribute `__name__`) to the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing['GrLivArea']\n",
    "y = housing['SalePrice']\n",
    "models = [model_1_simple_avg, model_2_avg_ppsf, model_3_bin_avg]\n",
    "model_sse = pd.Series({model.__name__: sse(X, y, model) for model in models})\n",
    "model_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These large numbers are actually quite difficult to compare by inspection. Let's use a bar plot to make the comparisons easier. Models two and three appear to be substantially better than model one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sse.plot(kind='bar', figsize=(8, 4), title='Sum of Squared Error for three models', rot=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root mean squared error\n",
    "\n",
    "One issue with the sum of squared error is that it is difficult to interpret. The number tends to be quite large and can increase with more data. As we saw above, it wasn't until we plotted the SSE that we could tell the differences between the models.\n",
    "\n",
    "The root mean squared error (RMSE) is a different but very similar error metric that we can use as our objective measure of performance. The calculation for RMSE begins like SSE by calculating the raw error and squaring it. But, instead of taking the sum, we take the mean. By calculating the mean instead of the sum, we get a **relative** measure of performance instead of **absolute**. In this instance, relative indicates a per-house average squared error. The square root of this mean is taken to produce the RMSE. Let's create a function to calculate the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(X, y, model):\n",
    "    y_pred = make_prediction(model, X)\n",
    "    error = y - y_pred\n",
    "    squared_error = error ** 2\n",
    "    mse = squared_error.mean()\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is now calculated for each model on all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rmse = pd.Series({model.__name__: rmse(X, y, model) for model in models})\n",
    "model_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These relative values are much easier to compare by inspection than the SSE. Still, it is helpful to visualize this new performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rmse.plot(kind='bar', figsize=(8, 4), title='Sum of Squared Error for three models', rot=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help remembering calculations\n",
    "\n",
    "The names sum of squared error and root mean squared error are descriptive and should help you remember how the metric is derived. The descriptions give you the order of calculations from right to left. For instance, the RMSE first takes the **E**rror, then **S**quares it, computes the **M**ean, and finally takes the square **R**oot.\n",
    "\n",
    "### SSE and RSME always rank models the same\n",
    "\n",
    "Although the SSE and RMSE error metrics produce different values, they always rank the models in the exact same order. For instance, if we built ten different models and calculated the ranking based on SSE, it would be the exact same as the ranking based on RMSE. Mathematically, the RMSE can be derived directly from the SSE by dividing it by the total number of points and then taking the square root. Let's verify this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(X, y, model_2_avg_ppsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(sse(X, y, model_2_avg_ppsf) / len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the RMSE just the mean absolute error?\n",
    "\n",
    "The mean absolute error (MAE) is simply the average of the absolute value of all the errors. The RMSE calculation produces a number that appears to be the MAE. This units of both of these error metrics are the original units of the target variable (dollars). The RMSE is NOT the MAE, though the two values will often be similar. Let's prove this with a simple example involving an array containing three errors, 10, 20, and 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = np.array([10, 20, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we calculate the MAE. It's unnecessary to include the `abs` function, but is presented here to match the exact definition of the MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(errors).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE produces a slightly higher result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt((errors ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic function for any error metric\n",
    "\n",
    "Below, we create a function that generalizes our procedure from above. You supply it the data, models, and the error metric function. It calculates a prediction and error for each input data for each model and then plots the single error metric for each model. It uses the name of the function as the title in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error(X, y, models, error_func):\n",
    "    errors = pd.Series({model.__name__: error_func(X, y, model) for model in models})\n",
    "    errors.plot(kind='bar', figsize=(8, 4), title=error_func.__name__, rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this out with our two error metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error(X, y, models, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error(X, y, models, sse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the best model\n",
    "From the plots above, it's clear that model 2 outperforms the others based on both error metrics. If our machine learning toolset were limited to just these models, we would choose model 2 as our final model and use it to make future predictions when new houses come on the market.\n",
    "\n",
    "### More complex models to come\n",
    "\n",
    "All of these models were built from scratch with a few lines of code. They were devised so that you could clearly see how a model was built, predictions made, and errors calculated. We will now move on to use more powerful and complex models. Instead of building them by hand will rely on the scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "<span  style=\"color:green; font-size:16px\">Create functions that calculate task performance for each of the following error metrics:</span>\n",
    "\n",
    "\n",
    "<span  style=\"color:green; font-size:16px\">\n",
    "    \n",
    "* Mean Absolute Error (MAE) - find the absolute error of each prediction, then return the mean of these values\n",
    "* Maximum Absolute Error - find the absolute error of each prediction, then return the maximum of these values\n",
    "* Mean of Cubed Error - find the absolute value of the cubed error of each, then return the mean of these values\n",
    "* Mean Logged Error - find the absolute error of each prediction, then take the natural logarithm of each, and return the mean of these values.\n",
    "* Come up with your own error metric\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"color:green; font-size:16px\">Use each function individually to get the performance measurement on model 3 from above. Then use the `plot_error` function to plot each error for each model.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
